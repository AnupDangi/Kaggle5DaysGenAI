{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:46:21.355198Z","iopub.execute_input":"2025-03-31T18:46:21.355454Z","iopub.status.idle":"2025-03-31T18:46:21.767717Z","shell.execute_reply.started":"2025-03-31T18:46:21.355420Z","shell.execute_reply":"2025-03-31T18:46:21.766708Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%pip install pandas\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:46:40.148932Z","iopub.execute_input":"2025-03-31T18:46:40.149339Z","iopub.status.idle":"2025-03-31T18:46:44.948619Z","shell.execute_reply.started":"2025-03-31T18:46:40.149301Z","shell.execute_reply":"2025-03-31T18:46:44.947229Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:51:43.625201Z","iopub.execute_input":"2025-03-31T18:51:43.625653Z","iopub.status.idle":"2025-03-31T18:51:50.476923Z","shell.execute_reply.started":"2025-03-31T18:51:43.625616Z","shell.execute_reply":"2025-03-31T18:51:50.475784Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:52:00.136055Z","iopub.execute_input":"2025-03-31T18:52:00.136446Z","iopub.status.idle":"2025-03-31T18:52:01.607619Z","shell.execute_reply.started":"2025-03-31T18:52:00.136415Z","shell.execute_reply":"2025-03-31T18:52:01.606772Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from google.api_core import retry\n\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\ngenai.models.Models.generate_content = retry.Retry(\n    predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:52:15.550627Z","iopub.execute_input":"2025-03-31T18:52:15.551228Z","iopub.status.idle":"2025-03-31T18:52:15.556229Z","shell.execute_reply.started":"2025-03-31T18:52:15.551190Z","shell.execute_reply":"2025-03-31T18:52:15.555168Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:52:29.824675Z","iopub.execute_input":"2025-03-31T18:52:29.825012Z","iopub.status.idle":"2025-03-31T18:52:29.940841Z","shell.execute_reply.started":"2025-03-31T18:52:29.824987Z","shell.execute_reply":"2025-03-31T18:52:29.939657Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\")\n\nprint(response.text)\n\n## IT gives the answer in rough format not in proper format.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:52:42.241207Z","iopub.execute_input":"2025-03-31T18:52:42.241557Z","iopub.status.idle":"2025-03-31T18:52:45.241694Z","shell.execute_reply.started":"2025-03-31T18:52:42.241530Z","shell.execute_reply":"2025-03-31T18:52:45.240625Z"}},"outputs":[{"name":"stdout","text":"Okay, imagine you have a really, REALLY smart puppy.  But instead of learning to fetch a ball, this puppy learns by looking at tons and tons of pictures or listening to tons of stories.\n\nAI, or Artificial Intelligence, is like that smart puppy's brain!  It's a computer program that can learn and make decisions based on the information it's given.\n\nHere's how it works:\n\n* **Lots and lots of examples:**  We show the AI tons of things, like pictures of cats and dogs, or examples of sentences.\n* **Spotting Patterns:**  The AI tries to find patterns in those examples. It might learn that cats usually have pointy ears and dogs have floppy ones.\n* **Making Predictions:**  Then, when you show the AI a *new* picture, it can use those patterns to guess if it's a cat or a dog!  That's like making a prediction.\n* **Getting Smarter Over Time:** If the AI guesses wrong, we can tell it!  Then it learns from its mistakes and gets better at guessing in the future.\n\nThink about it like this:\n\n* **Playing a video game:**  An AI can learn to play a video game really well by playing it over and over again and figuring out the best strategies.\n* **Helping you find things online:**  When you search for something on the internet, AI helps you find the right information based on what you type.\n* **Talking to you:**  Some AI can even understand what you say and talk back to you!  Think of things like Siri or Alexa.\n\nSo, AI is basically a super-smart computer program that can learn, make decisions, and help us with all sorts of things!  It's like a robot brain that's still learning and getting better every day!\n\nDo you have any questions about what AI does? Maybe we can talk about a specific example!\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"## convert into mardown format for the response\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:53:05.227120Z","iopub.execute_input":"2025-03-31T18:53:05.227535Z","iopub.status.idle":"2025-03-31T18:53:05.234745Z","shell.execute_reply.started":"2025-03-31T18:53:05.227504Z","shell.execute_reply":"2025-03-31T18:53:05.233727Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, imagine you have a really, REALLY smart puppy.  But instead of learning to fetch a ball, this puppy learns by looking at tons and tons of pictures or listening to tons of stories.\n\nAI, or Artificial Intelligence, is like that smart puppy's brain!  It's a computer program that can learn and make decisions based on the information it's given.\n\nHere's how it works:\n\n* **Lots and lots of examples:**  We show the AI tons of things, like pictures of cats and dogs, or examples of sentences.\n* **Spotting Patterns:**  The AI tries to find patterns in those examples. It might learn that cats usually have pointy ears and dogs have floppy ones.\n* **Making Predictions:**  Then, when you show the AI a *new* picture, it can use those patterns to guess if it's a cat or a dog!  That's like making a prediction.\n* **Getting Smarter Over Time:** If the AI guesses wrong, we can tell it!  Then it learns from its mistakes and gets better at guessing in the future.\n\nThink about it like this:\n\n* **Playing a video game:**  An AI can learn to play a video game really well by playing it over and over again and figuring out the best strategies.\n* **Helping you find things online:**  When you search for something on the internet, AI helps you find the right information based on what you type.\n* **Talking to you:**  Some AI can even understand what you say and talk back to you!  Think of things like Siri or Alexa.\n\nSo, AI is basically a super-smart computer program that can learn, make decisions, and help us with all sorts of things!  It's like a robot brain that's still learning and getting better every day!\n\nDo you have any questions about what AI does? Maybe we can talk about a specific example!\n"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"## Memory of chat this is for a single chat \nchat = client.chats.create(model='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:53:18.809717Z","iopub.execute_input":"2025-03-31T18:53:18.810031Z","iopub.status.idle":"2025-03-31T18:53:19.396801Z","shell.execute_reply.started":"2025-03-31T18:53:18.810006Z","shell.execute_reply":"2025-03-31T18:53:19.395894Z"}},"outputs":[{"name":"stdout","text":"Nice to meet you, Zlork! It's a pleasure to make your acquaintance. Is there anything I can help you with today?\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"response = chat.send_message('Can you tell me something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:53:35.136805Z","iopub.execute_input":"2025-03-31T18:53:35.137122Z","iopub.status.idle":"2025-03-31T18:53:36.617341Z","shell.execute_reply.started":"2025-03-31T18:53:35.137097Z","shell.execute_reply":"2025-03-31T18:53:36.616232Z"}},"outputs":[{"name":"stdout","text":"Okay, here's a fascinating fact about dinosaurs:\n\n**Many paleontologists now believe that many, if not most, dinosaurs were feathered!**\n\nWhile we often imagine dinosaurs as scaly reptiles, fossil evidence, particularly from China, has revealed that many species, including some theropods like *Velociraptor* (yes, even the movie star!), had feathers. These feathers weren't necessarily for flight. They could have been used for insulation, display, or even brooding eggs.\n\nThis discovery has revolutionized our understanding of dinosaur evolution and their connection to birds. In fact, modern birds are now widely accepted as direct descendants of theropod dinosaurs.\n\nSo, next time you picture a dinosaur, don't just imagine scales - think feathers! It makes them a lot more colorful and bird-like.\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"## recall from the active chat memory\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:53:46.452410Z","iopub.execute_input":"2025-03-31T18:53:46.452766Z","iopub.status.idle":"2025-03-31T18:53:47.021967Z","shell.execute_reply.started":"2025-03-31T18:53:46.452736Z","shell.execute_reply":"2025-03-31T18:53:47.020976Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Zlork.\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"## choose model select all model\nfor model in client.models.list():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:53:55.640949Z","iopub.execute_input":"2025-03-31T18:53:55.641331Z","iopub.status.idle":"2025-03-31T18:53:55.875072Z","shell.execute_reply.started":"2025-03-31T18:53:55.641299Z","shell.execute_reply":"2025-03-31T18:53:55.874232Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.5-pro-exp-03-25\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-exp-image-generation\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/gemma-3-27b-it\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/aqa\nmodels/imagen-3.0-generate-002\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from pprint import pprint\n\nfor model in client.models.list():\n  if model.name == 'models/gemini-2.0-flash':\n    pprint(model.to_json_dict())\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:07:23.698056Z","iopub.execute_input":"2025-03-31T19:07:23.698532Z","iopub.status.idle":"2025-03-31T19:07:23.930915Z","shell.execute_reply.started":"2025-03-31T19:07:23.698501Z","shell.execute_reply":"2025-03-31T19:07:23.930051Z"}},"outputs":[{"name":"stdout","text":"{'description': 'Gemini 2.0 Flash',\n 'display_name': 'Gemini 2.0 Flash',\n 'input_token_limit': 1048576,\n 'name': 'models/gemini-2.0-flash',\n 'output_token_limit': 8192,\n 'supported_actions': ['generateContent', 'countTokens'],\n 'tuned_model_info': {},\n 'version': '2.0'}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"## providing generation parameters with tokens size\nfrom google.genai import types\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:07:27.132089Z","iopub.execute_input":"2025-03-31T19:07:27.132505Z","iopub.status.idle":"2025-03-31T19:07:28.702373Z","shell.execute_reply.started":"2025-03-31T19:07:27.132471Z","shell.execute_reply":"2025-03-31T19:07:28.701466Z"}},"outputs":[{"name":"stdout","text":"## The Enduring Olive: A Cornerstone of Culture and Cuisine in Modern Society\n\nThe olive, a small drupe born from the ancient olive tree (Olea europaea), has transcended its simple biological definition to become a profound symbol of peace, prosperity, and longevity. While often relegated to the role of a garnish or a pizza topping, olives and their derived products, particularly olive oil, play a far more significant role in modern society than many realize. They are integral to culinary traditions, contribute to economic stability in olive-growing regions, offer a wealth of health benefits, and even contribute to environmental sustainability when cultivated responsibly. Understanding the enduring importance of olives requires examining their multifaceted impact on our lives.\n\nFirstly, the olive's culinary significance is undeniable and deeply ingrained in various cultures, particularly those bordering the Mediterranean Sea. For millennia, olives have been a staple food, preserved and prepared in countless ways, each reflecting the unique flavors and traditions of a specific region. From the briny Kalam\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"## essay generation\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a short poem on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:54:41.768522Z","iopub.execute_input":"2025-03-31T18:54:41.768844Z","iopub.status.idle":"2025-03-31T18:54:42.798507Z","shell.execute_reply.started":"2025-03-31T18:54:41.768819Z","shell.execute_reply":"2025-03-31T18:54:42.797506Z"}},"outputs":[{"name":"stdout","text":"From ancient groves, a gift remains,\nAn olive's oil, in sunlit strains.\nOn salads bright, or bread so warm,\nA flavor deep, defying storm.\n\nIn tapenades, a savory blend,\nA healthy fat, a faithful friend.\nFrom modern plates to ancient lore,\nThe olive tree, we all adore.\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"## Temparature of model context generation for randomness in token selection.\n\nhigh_temp_config = types.GenerateContentConfig(temperature=2.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:55:01.839772Z","iopub.execute_input":"2025-03-31T18:55:01.840175Z","iopub.status.idle":"2025-03-31T18:55:04.106671Z","shell.execute_reply.started":"2025-03-31T18:55:01.840117Z","shell.execute_reply":"2025-03-31T18:55:04.105759Z"}},"outputs":[{"name":"stdout","text":"Turquoise\n -------------------------\nMagenta\n -------------------------\nOrange\n -------------------------\nMagenta\n -------------------------\nMagenta.\n -------------------------\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"## regulating temparature and checking output with selection of tokens\nlow_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n  response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      config=low_temp_config,\n      \n      contents='Pick a random colour... (respond in a single word)')\n\n  if response.text:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:55:22.439496Z","iopub.execute_input":"2025-03-31T18:55:22.439815Z","iopub.status.idle":"2025-03-31T18:55:24.335649Z","shell.execute_reply.started":"2025-03-31T18:55:22.439790Z","shell.execute_reply":"2025-03-31T18:55:24.334793Z"}},"outputs":[{"name":"stdout","text":"Azure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\nAzure\n -------------------------\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"## Top P provides parameter to control the diversity of the model's output.\nmodel_config = types.GenerateContentConfig(\n    # These are the default values for gemini-2.0-flash.\n    temperature=1.0,\n    top_p=0.95,\n)\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:56:21.163907Z","iopub.execute_input":"2025-03-31T18:56:21.164304Z","iopub.status.idle":"2025-03-31T18:56:25.841846Z","shell.execute_reply.started":"2025-03-31T18:56:21.164276Z","shell.execute_reply":"2025-03-31T18:56:25.840851Z"}},"outputs":[{"name":"stdout","text":"Jasper, a ginger tabby with emerald eyes, considered himself a connoisseur of sunbeams and cardboard boxes. His life was a predictable, purr-fect symphony of naps and chin scratches. But one Tuesday, the mundane melody went wildly off-key.\n\nIt started with the butterfly. Not just any butterfly, mind you, but a shimmering, indigo creature with wings dusted in what looked suspiciously like glitter. It fluttered outside the window, beckoning Jasper with an almost palpable energy. He'd seen butterflies before, of course, but this one felt… different. It hummed with a silent promise of something more.\n\nDriven by an unprecedented surge of curiosity (and perhaps the allure of glitter), Jasper nudged the window latch open with his nose. It clicked! He squeezed through, the scent of jasmine and unknown adventures flooding his senses.\n\nHe found himself in Mrs. Higgins' meticulously manicured rose garden, a place strictly forbidden. Usually, the threat of a spray from her garden hose kept him at bay. Today, however, the indigo butterfly danced just beyond the rose bushes, leading him deeper into the forbidden floral labyrinth.\n\nThe garden was a wonderland. He stalked ladybugs crawling on velvety petals, batted at fat bumblebees buzzing in a hazy blur, and sniffed at roses that smelled like sunshine and secrets. He even managed to evade Mrs. Higgins' suspicious gaze by squeezing under a terracotta pot.\n\nThe butterfly led him past the garden, through a hole in the fence, and into the sprawling, untamed wilderness beyond. Jasper, a creature of comfort, hesitated. This was uncharted territory. But the glitter! He couldn't resist.\n\nThe journey was challenging. Thorns scratched his fur, muddy puddles splashed his paws, and a grumpy squirrel scolded him from a branch. He even encountered a large, slobbery dog who seemed more interested in playing than eating him (much to Jasper's relief).\n\nFinally, the butterfly landed on a gnarled oak tree, its branches reaching towards the sky like crooked fingers. At the base of the tree, nestled amongst the roots, was a small, wooden box.\n\nJasper approached cautiously. He nudged it with his nose. The box, miraculously, popped open. Inside, nestled on a bed of soft moss, was not treasure, not food, but a single, perfect feather, iridescent and shimmering just like the butterfly's wings.\n\nHe didn't know why it was there, or why the butterfly had led him on this quest. But as he touched the feather with his paw, a sense of peace settled over him. It was a symbol, he decided, of the adventure he had dared to undertake.\n\nThe sun began to set, casting long shadows across the wilderness. Jasper knew he had to return home. He carried the feather carefully in his mouth, its delicate weight a reminder of his bravery.\n\nBack in his own yard, he snuck back through the window, leaving only a faint scent of jasmine and the faint shimmer of glitter on the windowsill. He curled up in his favorite cardboard box, the feather tucked beside him.\n\nHe was still Jasper, connoisseur of sunbeams and cardboard boxes. But now, he was also Jasper, the adventurer. And as he drifted off to sleep, he dreamed of indigo butterflies and the endless possibilities that lay just beyond the windowsill. He knew, with a certainty that resonated in his purrs, that this was just the beginning of his extraordinary, purr-fectly unpredictable life.\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"## Maxing output tokens to get the specific one answer based on output token size \nmodel_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:16:16.714883Z","iopub.execute_input":"2025-03-31T19:16:16.715272Z","iopub.status.idle":"2025-03-31T19:16:17.221761Z","shell.execute_reply.started":"2025-03-31T19:16:16.715232Z","shell.execute_reply":"2025-03-31T19:16:17.220741Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"## Enum Mode to check the number of sentiments and provide the accurate among these \nimport enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ),\n    contents=zero_shot_prompt)\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:58:49.504199Z","iopub.execute_input":"2025-03-31T18:58:49.504568Z","iopub.status.idle":"2025-03-31T18:58:50.114022Z","shell.execute_reply.started":"2025-03-31T18:58:49.504540Z","shell.execute_reply":"2025-03-31T18:58:50.113049Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"enum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:58:59.193163Z","iopub.execute_input":"2025-03-31T18:58:59.193550Z","iopub.status.idle":"2025-03-31T18:58:59.198800Z","shell.execute_reply.started":"2025-03-31T18:58:59.193507Z","shell.execute_reply":"2025-03-31T18:58:59.197763Z"}},"outputs":[{"name":"stdout","text":"Sentiment.POSITIVE\n<enum 'Sentiment'>\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"## One show and few shot prompt \nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T18:59:11.705748Z","iopub.execute_input":"2025-03-31T18:59:11.706216Z","iopub.status.idle":"2025-03-31T18:59:12.336466Z","shell.execute_reply.started":"2025-03-31T18:59:11.706168Z","shell.execute_reply":"2025-03-31T18:59:12.335554Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"## converting context into a proper json format\nimport typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:18:29.440207Z","iopub.execute_input":"2025-03-31T19:18:29.440609Z","iopub.status.idle":"2025-03-31T19:18:30.211781Z","shell.execute_reply.started":"2025-03-31T19:18:29.440583Z","shell.execute_reply":"2025-03-31T19:18:30.210548Z"}},"outputs":[{"name":"stdout","text":"{\n  \"size\": \"large\",\n  \"ingredients\": [\"apple\", \"chocolate\"],\n  \"type\": \"dessert\"\n}\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"## Chain of Thoughts\nprompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:00:31.188851Z","iopub.execute_input":"2025-03-31T19:00:31.189302Z","iopub.status.idle":"2025-03-31T19:00:31.741362Z","shell.execute_reply.started":"2025-03-31T19:00:31.189267Z","shell.execute_reply":"2025-03-31T19:00:31.740486Z"}},"outputs":[{"name":"stdout","text":"52\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"## step by step explanation of chain of thoughts\nprompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:02:50.824287Z","iopub.execute_input":"2025-03-31T19:02:50.824772Z","iopub.status.idle":"2025-03-31T19:02:51.982545Z","shell.execute_reply.started":"2025-03-31T19:02:50.824734Z","shell.execute_reply":"2025-03-31T19:02:51.981591Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Here's how to solve this problem step-by-step:\n\n1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n\n2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3.  **Determine the partner's current age:** Since the age difference remains the same, your partner is currently 20 + 8 = 28 years old.\n\n**Answer:** Your partner is currently 28 years old."},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"## ReAct: Reason and act\nmodel_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:20:46.308213Z","iopub.execute_input":"2025-03-31T19:20:46.308639Z","iopub.status.idle":"2025-03-31T19:20:46.313822Z","shell.execute_reply.started":"2025-03-31T19:20:46.308608Z","shell.execute_reply":"2025-03-31T19:20:46.312727Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"## Works on three steps \n## Step1 thought\n## Step2 Action\n## Step3 Observation\nquestion = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:21:13.072605Z","iopub.execute_input":"2025-03-31T19:21:13.072961Z","iopub.status.idle":"2025-03-31T19:21:13.829302Z","shell.execute_reply.started":"2025-03-31T19:21:13.072930Z","shell.execute_reply":"2025-03-31T19:21:13.828518Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the transformers NLP paper and then find the youngest author listed on that paper.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:21:31.800862Z","iopub.execute_input":"2025-03-31T19:21:31.801236Z","iopub.status.idle":"2025-03-31T19:21:32.730893Z","shell.execute_reply.started":"2025-03-31T19:21:31.801204Z","shell.execute_reply":"2025-03-31T19:21:32.729936Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nNow I have a list of authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the youngest author from the list. I will have to search for their birthdates.\n\nAction 2\n<search>Ashish Vaswani birthdate</search>\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"## Thinking Mode -- explains the thinking process at last convert it ito markdown\nimport io\nfrom IPython.display import Markdown, clear_output\n\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:23:21.424498Z","iopub.execute_input":"2025-03-31T19:23:21.424871Z","iopub.status.idle":"2025-03-31T19:23:29.522325Z","shell.execute_reply.started":"2025-03-31T19:23:21.424842Z","shell.execute_reply":"2025-03-31T19:23:29.521272Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Based on publicly available information, **Aidan N. Gomez** appears to be the youngest author listed on the \"Attention is All You Need\" paper, which introduced the Transformer architecture.\n\nHere's why and how we can determine this:\n\n* **The Paper:** The paper you're referring to is likely \"Attention is All You Need\" by Vaswani et al., published in 2017. This is the foundational paper for the Transformer architecture that revolutionized NLP.\n* **Authors:** The authors are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n* **Determining Age (Approximate):**  It's difficult to get exact birthdates for all authors publicly. However, we can look at their academic backgrounds and career progression to get a sense of relative age.\n\n    * **Aidan N. Gomez:**  At the time of the paper's publication (2017), Aidan N. Gomez was a PhD student at the University of Oxford. PhD students are generally younger than established researchers or staff scientists.  His LinkedIn profile indicates he started his PhD in 2015, further suggesting he was early in his career.\n\n    * **Other Authors:**  Many of the other authors were already working as research scientists or engineers at Google Brain or other established research institutions at the time of publication. This generally implies they were further along in their careers and likely older than a PhD student.  For example, Noam Shazeer and Łukasz Kaiser were known researchers at Google Brain.\n\n**Conclusion:**\n\nWhile we don't have precise birthdates to confirm definitively, based on career stage at the time of publication, **Aidan N. Gomez** is highly likely to be the youngest author among the authors of \"Attention is All You Need.\"  Being a PhD student while the others were established researchers makes a strong case for him being the youngest.\n\nIt's important to note that \"youngest\" is relative and based on the information we can readily find. It's possible there are nuances we don't have access to. However, based on the available career stage indicators, Aidan N. Gomez is the most probable youngest author."},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"## Generating Code:\n# The Gemini models love to talk, so it helps to specify they stick to the code if that\n# is all that you want.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ),\n    contents=code_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:24:32.924696Z","iopub.execute_input":"2025-03-31T19:24:32.925045Z","iopub.status.idle":"2025-03-31T19:24:33.581698Z","shell.execute_reply.started":"2025-03-31T19:24:32.925018Z","shell.execute_reply":"2025-03-31T19:24:33.580669Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n```"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"## Code execution\nfrom pprint import pprint\n\nconfig = types.GenerateContentConfig(\n    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n)\n\ncode_exec_prompt = \"\"\"\nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\nfor part in response.candidates[0].content.parts:\n  pprint(part.to_json_dict())\n  print(\"-----\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:25:14.076953Z","iopub.execute_input":"2025-03-31T19:25:14.077337Z","iopub.status.idle":"2025-03-31T19:25:16.296317Z","shell.execute_reply.started":"2025-03-31T19:25:14.077303Z","shell.execute_reply":"2025-03-31T19:25:16.295463Z"}},"outputs":[{"name":"stdout","text":"{'text': \"Okay, I can do that. First, I'll generate the first 14 odd prime \"\n         'numbers. Remember that a prime number is a number greater than 1 '\n         'that has no positive divisors other than 1 and itself. Also, I will '\n         'need to exclude 2 from consideration as it is even, as requested. '\n         \"After I identify the first 14 odd prime numbers, I'll calculate \"\n         'their sum.\\n'\n         '\\n'}\n-----\n{'executable_code': {'code': 'primes = []\\n'\n                             'num = 3\\n'\n                             'while len(primes) < 14:\\n'\n                             '  is_prime = True\\n'\n                             '  for i in range(2, int(num**0.5) + 1):\\n'\n                             '    if num % i == 0:\\n'\n                             '      is_prime = False\\n'\n                             '      break\\n'\n                             '  if is_prime:\\n'\n                             '    primes.append(num)\\n'\n                             '  num += 2\\n'\n                             '\\n'\n                             \"print(f'{primes=}')\\n\"\n                             '\\n'\n                             'import numpy as np\\n'\n                             'sum_of_primes = np.sum(primes)\\n'\n                             \"print(f'{sum_of_primes=}')\\n\",\n                     'language': 'PYTHON'}}\n-----\n{'code_execution_result': {'outcome': 'OUTCOME_OK',\n                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n                                     '31, 37, 41, 43, 47]\\n'\n                                     'sum_of_primes=np.int64(326)\\n'}}\n-----\n{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n-----\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"## converting the previous responose into proper markdown format\nfor part in response.candidates[0].content.parts:\n    if part.text:\n        display(Markdown(part.text))\n    elif part.executable_code:\n        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n    elif part.code_execution_result:\n        if part.code_execution_result.outcome != 'OUTCOME_OK':\n            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n\n        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:26:17.767589Z","iopub.execute_input":"2025-03-31T19:26:17.767961Z","iopub.status.idle":"2025-03-31T19:26:17.779842Z","shell.execute_reply.started":"2025-03-31T19:26:17.767934Z","shell.execute_reply":"2025-03-31T19:26:17.778889Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Okay, I can do that. First, I'll generate the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has no positive divisors other than 1 and itself. Also, I will need to exclude 2 from consideration as it is even, as requested. After I identify the first 14 odd prime numbers, I'll calculate their sum.\n\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\nprimes = []\nnum = 3\nwhile len(primes) < 14:\n  is_prime = True\n  for i in range(2, int(num**0.5) + 1):\n    if num % i == 0:\n      is_prime = False\n      break\n  if is_prime:\n    primes.append(num)\n  num += 2\n\nprint(f'{primes=}')\n\nimport numpy as np\nsum_of_primes = np.sum(primes)\nprint(f'{sum_of_primes=}')\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```\nprimes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=np.int64(326)\n\n```"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\n"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"## Explaining Code\nfile_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T19:27:00.522222Z","iopub.execute_input":"2025-03-31T19:27:00.522648Z","iopub.status.idle":"2025-03-31T19:27:03.735695Z","shell.execute_reply.started":"2025-03-31T19:27:00.522617Z","shell.execute_reply":"2025-03-31T19:27:03.734809Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a Bash/Zsh script that customizes your command-line prompt to display Git repository information. In other words, it enhances your prompt to show things like the current branch, the status of your working directory (e.g., if there are uncommitted changes), and whether you're ahead or behind the remote repository.\n\nHere's a breakdown:\n\n*   **What it is:** It's a script designed to be sourced (loaded) into your shell (Bash or Zsh). It defines functions and variables that modify the `PS1` variable, which controls the appearance of your command prompt.\n*   **What it does:**\n    *   **Displays Git information:**  It fetches information about the current Git repository (if you're in one) and formats it into a string. This includes the branch name, status of staged/unstaged changes, and remote tracking status.\n    *   **Customizes the prompt:**  It allows you to configure the colors, symbols, and information displayed in the prompt based on various Git states.\n    *   **Supports theming:** It allows to change the look and feel of your prompt using different themes.\n    *   **Handles virtual environments:**  It can optionally display the active Python virtual environment.\n\n*   **Why you'd use it:**\n    *   **Quickly see Git status:**  Instead of running `git status` all the time, you can see the status at a glance directly in your prompt.\n    *   **Improve workflow:**  Knowing the branch and status helps you stay oriented within your Git repositories and avoid mistakes.\n    *   **Make your terminal more informative:**  It adds context to your command line, making it easier to understand your environment.\n    *   **Personalize your terminal:** The level of customization makes it possible to personalize the command prompt to a level that is perfect for you.\n\nIn short, it's a convenience tool that improves your Git workflow by providing real-time Git status information directly in your shell prompt.\n"},"metadata":{}}],"execution_count":44}]}